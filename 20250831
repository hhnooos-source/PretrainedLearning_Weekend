from PIL import Image
import numpy as np
import glob

dirNames = ['Aiden', 'Andrew', 'Cathy']

# folder중 최대 해상도 찾기
widthRatio = []
heightRatio = []
for name in dirNames:
    for file in sorted(glob.glob(f"/kaggle/input/data-face1/{name}/image*.jpg")):
        img = np.array(Image.open(file))
        widthRatio.append(img.shape[1])
        heightRatio.append(img.shape[0])

print('너비 최대 해상도 :', np.max(widthRatio))
print('높이 최대 해상도 :', np.max(heightRatio))

# 작업 폴더 생성
import os
os.mkdir("./Face")
os.mkdir("./Face/Gray")

# 이름별 directory 생성
for name in dirNames:
    os.mkdir(f"./Face/Gray/{name}")

for name in dirNames:
    fileCount = 0
    for file in sorted(glob.glob(f"/kaggle/input/data-face1/{name}/image*.jpg")):
        img = Image.open(file)
        imgResize = img.convert('L')
        imgArray = np.array(imgResize)

        imgDummy = np.zeros(400*300).reshape(400, 300)
        rowNum = (400 - imgArray.shape[0]) / 2
        colNum = (300 - imgArray.shape[1]) / 2

        k = 0
        for i in range(int(rowNum), int(rowNum)+imgArray.shape[0]):
            l = 0
            for j in range(int(colNum), int(colNum)+imgArray.shape[1]):
                imgDummy[i,j] = imgArray[k,l]
                l+=1
            k+=1
        img2 = Image.fromarray(imgDummy.astype('uint8'), 'L')
        img2.save(f"/kaggle/working/Face/Gray/{name}/image_{fileCount:04d}.jpg", "JPEG")
        fileCount+=1


number_of_data = 18 * len(dirNames)
img_width_size = 300
img_height_size = 400

train_data = np.zeros(number_of_data*img_width_size*img_height_size).reshape(number_of_data, img_height_size, img_width_size)

i = 0
for name in dirNames:
    for file in sorted(glob.glob(f"/kaggle/working/Face/Gray/{name}/*.jpg")):
        img = np.array(Image.open(file))
        train_data[i,:,:] = img
        i+=1

train_data.shape

import matplotlib.pyplot as plt

plt.imshow(train_data[20], cmap='gray')
plt.show()

# 여러개 이미지를 같이 보기
plt.figure(figsize=(20, 20))
orderNo = range(0, len(dirNames)*18, 18)

for i in range(1, len(dirNames)+1):
    plt.subplot(1, len(dirNames), i)
    plt.imshow(train_data[orderNo[i-1]].reshape(400, 300), cmap='gray')
    plt.title(dirNames[i-1])


a = [0 for _ in range(18)]
[num for num in range(3)]

a = [0 for _ in range(18)]
b = [1 for _ in range(18)]
c = [2 for _ in range(18)]

target_data = a + b + c
print(target_data)

target_data = []
for num in range(3):
    for _ in range(18):
        target_data.append(num)

print(target_data)

target_data = [num for num in range(3) for _ in range(18)]
print(target_data)

import torch

train_input = torch.tensor(train_data)
train_target = torch.tensor(target_data)

print(train_data.shape)
print(train_target.shape)

from sklearn.model_selection import train_test_split

train_input = train_input.unsqueeze(1).float() / 255.0 # 채널 차원 추가 및 정규화
train_input, val_input, train_target, val_target = \
                    train_test_split(
                        train_input,
                        train_target,
                        test_size=0.2,
                        random_state=42
                    )



# dimension 확인
print(train_input.shape, train_target.shape)
print(val_input.shape, val_target.shape)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

# Dataset 및 DataLoader 생성
batch_size = 32 # mini batch
train_dataset = TensorDataset(train_input, train_target)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

val_dataset = TensorDataset(val_input, val_target)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64 * 100 * 75, 128) 
        # 이미지 입력 크기가 (1, 400, 300)인데, MaxPooling 2번 반복 후 fc1 입력 크기가 64 x 100 x 75가 되어야 하므로
        self.relu3 = nn.ReLU()
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, 3)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool2(x)

        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu3(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x 

# 손실함수와 옵티마이저
model = CNNModel()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# 학습 함수 정의
def train(model, train_loader, criterion, optimizer, device):
    model.train() # 뉴런들이 훈련모드로 동작하도록 설정 
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad() # 이전 반복에서 계산된 그래디언트를 초기화 
        outputs = model(inputs) # 내부적으로 model.forward(inputs) 호출하여 예측값 얻음 
        loss = criterion(outputs, targets) # 모델의 예측값과 실제 타겟 간의 손실을 계산 
        loss.backward() # 손실에 대한 그래디언트를 계산하고 역전파 
        optimizer.step() # 계산된 그래디언트를 사용하고 모델의 파라미터를 업데이트 
    return loss.item() # 현재 배치의 손실값을 반환 

# 평가함수 
def evaluate(model, val_loader, criterion, device):
    model.eval() # 평가모드로 동작하도록 설정 
    total_loss = 0 # 전체 손실 합계 
    correct = 0 # 정확하게 예측한 샘플 수 
    total = 0 # 전체 샘플 수
    with torch.no_grad() : # 평가 중에는 그래디언트 계산이 필요 없으므로 메모리 사용량 및 연산 속도 향상
        for inputs, targets in val_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs) # 예측값 발생 
            loss = criterion(outputs, targets) # 손실계산
            total_loss += loss.item() # 전체 손실에 더하기 
            _, predicted = outputs.max(1) # 전체 샘플에 대해 가장 높은 확률을 가진 인덱스 
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item() # 예측값과 실제값이 일치하는 경우 True를 발생 
        return total_loss/len(val_loader), correct/total # 평균손실과 정확도 

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
model.to(device)

# 훈련 반복
num_epochs = 50

for epoch in range(num_epochs):
    train_loss = train(model, train_loader, criterion, optimizer, device)
    print(f"Epoch[{epoch+1}/{num_epochs}], Loss :{train_loss:.4f}")
